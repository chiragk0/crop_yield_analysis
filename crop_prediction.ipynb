{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Yield Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessarty libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data set\n",
    "yield_df = pd.read_csv('/Users/chirag/Downloads/yield_df.csv')\n",
    "yield_df = yield_df.drop(yield_df.columns[0], axis=1)\n",
    "yield_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a top-down exploratory data analysis of the yield dataset that covers a multitude of key statistics\n",
    "profile = ProfileReport(yield_df, title = \"Exploratory Data Analysis of Yield Data\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> According to the correlation heatmap and the corresponding table, there are no correlations between any of the variables in the yield dataframe. We also don't have any missing values in any our columns. Nice! In addition, we have 2101 duplicate rows which accounts for approximately 7.4% of all the rows. Some of the duplicate even show up 4 times, indicating there is a combination of factors which allow for the same results to show up time after time. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a more concise look at key statistics\n",
    "yield_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the 101 countries by the 10 highest yield productions\n",
    "yield_df.groupby(['Area'],sort=True)['hg/ha_yield'].sum().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the number of items in each area by the yield\n",
    "yield_df.groupby(['Item','Area'],sort=True)['hg/ha_yield'].sum().nlargest(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data for Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I chose to employ one-hot encoding to convert our catagorical variables (Area, Item) into numerical data so they can be used in our machine-learning model of choice. The encoding creates a binary column for each category and returns a matrix with the results.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_df_onehot = pd.get_dummies(yield_df, columns=['Area',\"Item\"], prefix = ['Country',\"Item\"])\n",
    "features=yield_df_onehot.loc[:, yield_df_onehot.columns != 'hg/ha_yield']\n",
    "label=yield_df['hg/ha_yield']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the year should not have much of an effect on the model so I chose to drop it\n",
    "features = features.drop(['Year'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our dataset contains features that highly vary in magnitudes, units, and range. Features with higher magnitudes will most likely have a heavier weight in our distance calculations when running our machine learning models (Random Forest, Gradient Boosting, and Decision Tree). Here, I ensure that all of the features are of the same magnitude via scaling. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "features=scaler.fit_transform(features) \n",
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> To train our machine learning algorithms, let's split the dataset into two (training and testing datsets). Since we want the training set to have as much data as possible, a 70/30 or 80/20 split in favor of the training set is desireable. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(features, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model):\n",
    "    model_name = model.__class__.__name__\n",
    "    fit=model.fit(train_data,train_labels)\n",
    "    y_pred=fit.predict(test_data)\n",
    "    r2 = r2_score(test_labels,y_pred)\n",
    "    msq = mean_squared_error(test_labels,y_pred, squared = False) #setting the 'argument' to false return the RMSE\n",
    "    return([model_name,r2,msq])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> First, we retrieve the name of the model by calling the '__class__.__name__' attribute, which returns the name of the class of the input model object. The function then fits the model object to a training dataset 'train_data' with corresponding labels 'train_labels', using the 'fit()' method of the model. Once the model is fit, the function generates predictions for a test dataset 'test_data' using the predict() method, and calculates the R-squared (coefficient of determination) score between the predicted and actual labels using the 'r2_score()' function. Finally, the function returns a list containing the name of the model and the R-squared score as computed by 'r2_score()'.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "     GradientBoostingRegressor(n_estimators=200, max_depth=3, random_state=0),\n",
    "     RandomForestRegressor(n_estimators=200, max_depth=3, random_state=0),\n",
    "    svm.SVR(),\n",
    "   DecisionTreeRegressor()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains each machine learning model in the models list using the compare_models function\n",
    "model_train=list(map(compare_models,models)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*model_train, sep = \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In our result above, we see the name of the algorithm next to its respective R^2 score, which is a statistical measure that represents the proportion of variance in the dependent variable (i.e., the output or target variable in a supervised learning problem) that is explained by the independent variables. Here, we use it as a measure of how well the model fits the data. The R^2 score ranges from 0 to 1, with a higher value indicating a better fit of the model to the data. A score of 1 indicates that the model perfectly fits the data, while a score of 0 indicates that the model does not explain any of the variance in the dependent variable. Using these guidelines, we can see that the Decision Tree Regressor model fits the data much better than any of our other models.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> To the right of our R^2 score lies our RMSE (Root Mean Squared Error) for each model. It measures the difference between the predicted values and the actual values of the target variable and is expressed in the same units as our target variable. A lower RMSE indicates that the model has better predictive power and is more accurate. The motivation behind calculating both R^2 and RMSE was to understand the performance of each model in a more comprehensive manner and to better justify choosing one model over the others. As such, the Decision Tree Regressor Model has the lowest RMSE score, making it the most ideal model for our data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_df_onehot = yield_df_onehot.drop(['Year'], axis=1)\n",
    "yield_df_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame(test_data,columns=yield_df_onehot.loc[:, yield_df_onehot.columns != 'hg/ha_yield'].columns) \n",
    "\n",
    "cntry=test_df[[col for col in test_df.columns if 'Country' in col]].stack()[test_df[[col for col in test_df.columns if 'Country' in col]].stack()>0]\n",
    "cntrylist=list(pd.DataFrame(cntry).index.get_level_values(1))\n",
    "countries=[i.split(\"_\")[1] for i in cntrylist]\n",
    "itm=test_df[[col for col in test_df.columns if 'Item' in col]].stack()[test_df[[col for col in test_df.columns if 'Item' in col]].stack()>0]\n",
    "itmlist=list(pd.DataFrame(itm).index.get_level_values(1))\n",
    "items=[i.split(\"_\")[1] for i in itmlist]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop([col for col in test_df.columns if 'Item' in col],axis=1,inplace=True)\n",
    "test_df.drop([col for col in test_df.columns if 'Country' in col],axis=1,inplace=True)\n",
    "\n",
    "test_df['Country']=countries\n",
    "test_df['Item']=items\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeRegressor()\n",
    "model=clf.fit(train_data,train_labels)\n",
    "\n",
    "test_df[\"yield_predicted\"]= model.predict(test_data)\n",
    "test_df[\"yield_actual\"]=pd.DataFrame(test_labels)[\"hg/ha_yield\"].tolist()\n",
    "test_group=test_df.groupby(\"Item\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Here, we create an instance of a decision tree regressor model. We then fit the model to the training data and labels, and stores the resulting trained model in the 'model' variable. Afterwards we predict the yield for the test dataset using the trained model and stores the predicted yield values in a new column called \"yield_predicted\" in the 'test_df' dataframe. In order to compare the predicted values to the actual values, we extract the actual yield values from the test labels dataframe and store them in a new column called \"yield_actual\" in the test_df dataframe. The last line groups the test_df dataframe by the \"Item\" column.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.apply(lambda x: r2_score(x.yield_actual,x.yield_predicted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> This calculates the R-squared score for each group (i.e., each crop) using the predicted and actual yield values, and returns a pandas Series object with the R-squared score for each group.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.apply(lambda x: mean_squared_error(x.yield_actual,x.yield_predicted, squared = False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> This computes the root mean squared error (RMSE) for each group in the test_df dataframe, where the groups are defined by the \"Item\" column. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(test_df, x=\"yield_actual\", y=\"yield_predicted\")\n",
    "fig.update_traces(marker=dict(color=\"Black\", size=3, opacity=0.5))\n",
    "fig.update_layout(\n",
    "    title=\"Actual vs Predicted\",\n",
    "    xaxis_title=\"Actual\",\n",
    "    yaxis_title=\"Predicted\",\n",
    "    font=dict(size=12),\n",
    "    margin=dict(l=50, r=50, b=50, t=50, pad=4)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_r_squared(y, yhat, X):\n",
    "    n = len(y)\n",
    "    k = X.shape[1]\n",
    "    r2 = r2_score(y, yhat)\n",
    "    adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - k - 1)\n",
    "    return adj_r2\n",
    "\n",
    "test_group.apply(lambda x: adjusted_r_squared(x.yield_actual,x.yield_predicted,x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The adjusted R-squared is a modified version of R-squared that adjusts for the number of input features used in the model, and it provides a more accurate measure of the model's goodness of fit.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances of the model\n",
    "imp = model.feature_importances_\n",
    "# Get the names of the features, excluding the target column \"hg/ha_yield\"\n",
    "names = yield_df_onehot.columns[yield_df_onehot.columns!=\"hg/ha_yield\"]\n",
    "# Create a dictionary with the feature importances and names\n",
    "varimp = {'imp': imp, 'names': names}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure A - \"Feature Importance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(varimp)\n",
    "df.sort_values(ascending=False,by=[\"imp\"],inplace=True)\n",
    "df=df.dropna()\n",
    "\n",
    "fig = px.bar(df, x='imp', y='names', orientation='h', color='imp',\n",
    "             color_continuous_scale='RdBu', title='Feature Importance')\n",
    "\n",
    "\n",
    "fig.update_layout(xaxis_title='Importance', yaxis_title='Features', \n",
    "                  xaxis_tickformat=',.0%', height=600, margin=dict(l=200, r=50, t=50, b=50))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure B - \"Top 7 Most Important Factors Affecting Crp Yield\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df.sort_values('imp', ascending = False).head(7),\n",
    "             x='imp',\n",
    "             y='names',\n",
    "             orientation='h',\n",
    "             color='imp',\n",
    "             color_continuous_scale='RdBu_r')\n",
    "fig.update_layout(title='Top 7 Most Important Factors Affecting Crop Yield')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure C - \"Yield for Each Item\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(yield_df, x='Item', y='hg/ha_yield', color='Item', color_discrete_sequence=px.colors.qualitative.Vivid)\n",
    "fig.update_layout(title='Yield for Each Item', xaxis_title='Item', yaxis_title='Yield (hg/ha)')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> From figure B, we see that crops being potatoes have the highest importance in the decision making for the model, especially since that it has the highest yield out of all the crops regardless of any other factors. The tonnage of pesticides beat out the average mm of rainfall per year as the most important external factor to crop growth. This was surprising because pesticides actually have negative effects on crop physiology―especially on photosynthesis―leading to a potential decrease in both the growth and the yield of crops. But it is no surpise that crops grown in India have the largest yield since India does have the largest crop sum in the entire dataset.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
